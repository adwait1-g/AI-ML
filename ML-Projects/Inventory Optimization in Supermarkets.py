# -*- coding: utf-8 -*-
"""Supermarket Challenge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qS4hHt4g0DcGvCcitPMD2vfDPqCDfH21

# Supermarket Inventory optimization challenge

A supermarket franchise in Germany was seeing a discrepencies in the actual inventory count of items in the shelves and the computer count. It was costing them a lot for manual recounting, they did not know which goods to recount and which not to, so they used to do a recount of almost all the items regularly.

This project uses machine learning to help the supermarket to see which item to count and which not to, thereby optimizing on time and capital.
"""

import pandas as pd

df = pd.read_csv("https://www.danielpesch.com/data_supermarket_challenge_test.csv")

import copy

len(df)

df.columns

import numpy as np

# Encode all the variables into numerical data

# Date-Time
df['datetime'] = pd.to_datetime(df['date'])
df['year'] = df['datetime'].dt.year
df['month_sin'] = df['datetime'].dt.month.apply(lambda x: np.sin(x * (2 * np.pi / 12)))
df['day_sin'] = df['datetime'].dt.day.apply(lambda x: np.sin(x * (2 * np.pi / 31)))

df.drop('datetime', axis=1, inplace=True)
df.drop('date', axis=1, inplace=True)

df.iloc[0]

# Categories
df = pd.get_dummies(df, columns=['category.1'])

df = pd.get_dummies(df, columns=['product_group_department'])

df.iloc[0]

# Get the dummy variables for Store format
df = pd.get_dummies(df, columns=['store_format'])

true_labels = df['stock_change']
test_data = df.drop('stock_change', axis=1)
# df.drop('stock_change', axis=1, inplace=True)
# test_data = df

test_data.head()
len(test_data)

predictions = rf_classifier.predict(test_data)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Calculate accuracy
accuracy = accuracy_score(true_labels, predictions)
print("Accuracy:", accuracy)

# Generate confusion matrix
conf_matrix = confusion_matrix(true_labels, predictions)
print("Confusion Matrix:\n", conf_matrix)

# Generate classification report
class_report = classification_report(true_labels, predictions)
print("Classification Report:\n", class_report)

"""### Building a random-forest based classifier."""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

X = df.drop('stock_change', axis=1)
y = df['stock_change']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 1. Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust test_size and random_state as needed

# 2. Create a Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # Adjust hyperparameters as needed

# 3. Train the model
rf_classifier.fit(X_train, y_train)

# 4. Make predictions on the test set
y_pred = rf_classifier.predict(X_test)

# 5. Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(1024, activation='relu', input_shape=(73,)),  # Input layer with 256 neurons
    layers.Dense(1024, activation='relu'),  # Hidden layer with 128 neurons
    layers.Dense(512, activation='relu'),  # Hidden layer with 64 neurons
    layers.Dense(128, activation='relu'),  # Hidden layer with 64 neurons
    layers.Dense(128, activation='relu'),  # Hidden layer with 64 neurons
    layers.Dense(1)  # Output layer with 1 neuron (for regression)
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Or categorical_crossentropy for multi-class

history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)