{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rToK0Tku8PPn"
      },
      "source": [
        "## makemore: becoming a backprop ninja\n",
        "\n",
        "Starting with the notebook shared by AK in lecture video description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sFElPqq8PPp"
      },
      "outputs": [],
      "source": [
        "# there no change change in the first several cells from last lecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChBbac4y8PPq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ],
      "metadata": {
        "id": "x6GhEWW18aCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124dc88a-c7e9-40b2-cc93-bd5e169259f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-14 19:44:05--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "\rnames.txt             0%[                    ]       0  --.-KB/s               \rnames.txt           100%[===================>] 222.80K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-04-14 19:44:06 (6.11 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klmu3ZG08PPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "120798cd-bb2f-4600-a5d6-c8151f09dbf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32033\n",
            "15\n",
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
          ]
        }
      ],
      "source": [
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))\n",
        "print(words[:8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCQomLE_8PPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1287ace-b73a-4d60-8a38-e96e8b12358a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ],
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_zt2QHr8PPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf865bee-c3d5-4091-fb71-0bfdb7195ad8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ],
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg20-vsg8PPt"
      },
      "outputs": [],
      "source": [
        "# ok biolerplate done, now we get to the action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJPU8HT08PPu"
      },
      "outputs": [],
      "source": [
        "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlFLjQyT8PPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7b0b4dc-ae83-46cf-f8ec-5690807de982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4137\n"
          ]
        }
      ],
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "# Note: I am initializating many of these parameters in non-standard ways\n",
        "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass.\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY-y96Y48PPv"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "n = batch_size # a shorter variable also, for convenience\n",
        "# construct a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ofj1s6d8PPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a1c1941-b1ef-4fd5-d562-602d07adb25c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.3493, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
        "\n",
        "emb = C[Xb] # embed the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff**2\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zH8jg9QCnh25",
        "outputId": "00b63d94-a3a6-43d2-8617-e07d44a4c100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO-8aqxK8PPw"
      },
      "outputs": [],
      "source": [
        "# Exercise 1: backprop through the whole thing manually,\n",
        "# backpropagating through exactly all of the variables\n",
        "# as they are defined in the forward pass above, one by one\n",
        "\n",
        "def my_backward():\n",
        "\n",
        "  # This is how I am going about doing this:\n",
        "  #\n",
        "  # 1. Maintain a running derivative, which notes the chain-ed derivative\n",
        "  #    until that particular layer.\n",
        "  # 2. At every step, for every variable, we stop and find the local derivative.\n",
        "  # 3. Gradient of variable = running-derivative multipled with local-derivative.\n",
        "  #=\n",
        "  # Problem: While I had conceptually understood how it works, trying to get\n",
        "  # the derivative right requires the exact, fine details - understanding what\n",
        "  # is going on, the PyTorch concepts like broadcasting etc., So, I am struggling\n",
        "  # through this one.\n",
        "  # For every variable, I will be writing possibly long notes on what exactly\n",
        "  # is going on with that variable, so that I myself will get clarity to get\n",
        "  # the derivative right.\n",
        "  # Maybe this was AK's intention, idk.\n",
        "\n",
        "  # 1. loss\n",
        "  # Straight-forward, dloss/dloss = 1.0\n",
        "  dloss = 1.0\n",
        "\n",
        "  # 2. logprobs\n",
        "  # loss = -logprobs[range(n), Yb].mean()\n",
        "  # What is happening with logprobs?\n",
        "  # a. probs is a (32, 27) output tensor. There is a row of probabilities for each of\n",
        "  #    the 32 samples. Each row is probabilities of the 27 characters in our dataset.\n",
        "  # b. probabilities are very small numbers at this point, so get their logs and\n",
        "  #    work with their logs => logprobs is also a (32, 27) tensor with each element\n",
        "  #    in logprobs equal to the natural logarithm of the corresponding element in\n",
        "  #    prob tensor.\n",
        "  # c. How do we get from logprobs to loss? Yb is a simple (1, 32) tensor, with\n",
        "  #    the desired outputs of each of the 32 input samples under process.\n",
        "  #    If 'a' is the desired output of 0th input sample, we get the log-probability\n",
        "  #    of 'a' from logprobs[0]. If 'e' is the desired output of the 1st sample, we\n",
        "  #    get the log-probability of 'e' from logprobs[1]. And this goes on until we\n",
        "  #    get the log-probabilities of desired-output of each of the 32 input samples.\n",
        "  # d. The mean of log-probabilities of desired-outputs is computed - which is\n",
        "  #    a scalar quantity, the loss.\n",
        "  # e. In particular, logprobs[range(n), Yb] is a simple (1, 32) tensor of all these\n",
        "  #    32 log-probabilities. The .mean() gets the mean of them.\n",
        "  #\n",
        "  # f. Now, let us get onto computing the gradient of logprobs (dlogprobs/dloss)\n",
        "  # g. loss = =logprobs[range(n), Yb].mean()\n",
        "  # h. logprobs is (32, 27) tensor. Out of all these 32 x 27 values, only\n",
        "  #    one in each of the 32 rows is used to compute the loss => Only the ones\n",
        "  #    that are the log-probabilities of desired-outputs contribute to loss, rest\n",
        "  #    are not contributing => the gradient of the rest of them now will be 0.\n",
        "  #    What will be the gradients of the ones which are contributing to loss?\n",
        "  # i. y = (x1 + x2 + x3)/3. dy/dx1 = 1/3. In essence, gradient of each of the 32\n",
        "  #    numbers will simply be 1/n, that is 1/32 in this case.\n",
        "  dlogprobs = torch.zeros_like(logprobs)\n",
        "  dlogprobs[range(n), Yb] = -1/n\n",
        "  cmp('logprobs', dlogprobs, logprobs)\n",
        "\n",
        "  # Note that we take a (32, 27) zero tensor and then set the cells that are\n",
        "  # contributing to the loss to -1/32.\n",
        "\n",
        "  # 3. probs\n",
        "  # logprobs = probs.log()\n",
        "  # a. logprobs is simply the natural logarithm of probabilities.\n",
        "  # b. probs is a (32, 27) tensor, and every element is contributing to logprobs.\n",
        "  # c. Its local derivative dlogprobs/dprobs will also be a (32, 27) tensor.\n",
        "  # d. d(logx) = 1/x, local derivative dlogprobs/dprobs = (1/probs)\n",
        "  dprobs = (1.0 / probs) * dlogprobs\n",
        "  cmp('probs', dprobs, probs)\n",
        "\n",
        "  # 4. counts_sum_inv\n",
        "  # probs = counts * counts_sum_inv\n",
        "  # counts ->         (32, 27)\n",
        "  # counts_sum_inv -> (32,  1)\n",
        "  # Local derivative: dprobs/dcounts_sum_inv\n",
        "  # Going by the scalar derivative rule, dprobs/dcounts_sum_inv = counts\n",
        "  # So, that way, actual gradient that way will be counts * dprobs,\n",
        "  # which will result in a (32, 27) tensor. - I don't think that is correct.\n",
        "  #\n",
        "  # I think the way that tensor multiplication is working is that the\n",
        "  # (32, 1) tensor is converted/broadcasted into a (32, 27) tensor, then\n",
        "  # an element by element multiplication happens (that is normal).\n",
        "  #\n",
        "  # Okay, let me go step by step.\n",
        "  # 1. counts_sum_inv (32, 1) is broadcasted into (32, 27) tensor.\n",
        "  # 2. Multiplication of counts (32, 32) with broadcasted (32, 27) tensor is computed\n",
        "  #\n",
        "  # Local derivative for this multiplication operation is \"counts\". But how do we\n",
        "  # deal with the broadcasting here? Because dcounts_sum_inv cannot be a (32, 27) tensor.\n",
        "  # It needs to be a (32, 1) tensor like counts_sum_inv.\n",
        "  #\n",
        "  # Okay, what I understand from reading up is when broadcasting from (32, 1) to\n",
        "  # (32, 27) happens, each duplicate element (in a row) contributes to the gradient.\n",
        "  # So, we need to sum up all the 27 contributions in each row giving rise to a\n",
        "  # (32, 1) tensor.\n",
        "  #\n",
        "  # Note that the sum is taken on the final derivative, not on local derivative.\n",
        "  #\n",
        "  dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
        "  cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "\n",
        "  # 5. counts_sum\n",
        "  # counts_sum_inv = counts_sum**-1\n",
        "  # counts_sum_inv -> (32, 1)\n",
        "  # counts_sum     -> (32, 1)\n",
        "  # A simple element by element operation.\n",
        "  # d (1/x)/dx = -1/(x**2) <- can be applied directly.\n",
        "  # Derivative will also be a (32, 1) tensor.\n",
        "  # Local Derivative = dcounts_sum_inv/dcounts_sum = -1/(counts_sum**2)\n",
        "  dcounts_sum = (-1.0 / (counts_sum**2)) * dcounts_sum_inv\n",
        "  cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "\n",
        "  # 6. counts\n",
        "  # counts is used in two places:\n",
        "  # a. probs = counts * counts_sum_inv\n",
        "  # b. counts_sum = counts.sum(1, keepdim = True)\n",
        "  # We need to find gradients in both cases. And remember saying these\n",
        "  # gradients get added, so we'll need to sum them up.\n",
        "  # a. probs = counts * counts_sum_inv\n",
        "  # probs ->       (32, 27)\n",
        "  # counts ->      (32, 27)\n",
        "  # counts_sum_inv (32,  1)\n",
        "  dcounts1 = (counts_sum_inv * dprobs)\n",
        "\n",
        "  # b. counts_sum = counts.sum(1, keepdim=True)\n",
        "  # counts_sum -> (32,  1)\n",
        "  # counts     -> (32, 27)\n",
        "  # y = (x1 + x2 + x3), so dy/dx1 = 1.0, each element contributes a unit gradient.\n",
        "  # I think the same can be applied here.\n",
        "  # In a row (with 27 variables in it), all of them are summed, so each of the 27\n",
        "  # variables is contributing unit gradient.\n",
        "  # Local Derivative = dcounts_sum/dcounts = A (32, 27) unit tensor\n",
        "  dcounts2 = torch.ones_like(counts) * dcounts_sum\n",
        "\n",
        "  # Now, both the gradients get added (really need to check the math behind this)\n",
        "  dcounts = dcounts1 + dcounts2\n",
        "  cmp('counts', dcounts, counts)\n",
        "\n",
        "  # 7. norm_logits\n",
        "  # counts = norm_logits.exp()\n",
        "  # norm_logits -> (32, 27)\n",
        "  # counts ->      (32, 27)\n",
        "  # Simple, element by element exponentiation.\n",
        "  # d(e**x)/dx = e**x\n",
        "  # Local derivative = dcounts/dnorm_logits = counts\n",
        "  dnorm_logits = counts * dcounts\n",
        "  cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "\n",
        "  # 8. logits_maxes\n",
        "  # a. norm_Logits = logits - logitmaxes\n",
        "  # norm_logits   -> (32, 27)\n",
        "  # logit_maxes   -> (32,  1) --> (32, 27) broadcasted tensor\n",
        "  #\n",
        "  # The derivative on the broadcasted (32, 27) tensor is negative-unit tensor\n",
        "  # of (32, 27) dimensions.\n",
        "  # Then we sum the gradients in each of the rows to give a (32, 1) gradient tensor.\n",
        "  # a. Start with a (32, 27) unit tensor.\n",
        "  # b. Multiply it with dnorm_digits (32, 27) tensor to get the gradients.\n",
        "  # c. Sum the rows to give a (32, 1) gradient tensor.\n",
        "  # d. Finally multiply by a -1.0 (because norm_logits = logitx - logit_maxes)\n",
        "  dlogit_maxes = (-1.0) * (torch.ones_like(logit_maxes) * dnorm_logits).sum(1, keepdim=True)\n",
        "  cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "\n",
        "  # 9. logits\n",
        "  # a. norm_logits = logits - logit_maxes\n",
        "  # b. logit_maxes = logits.max(1, keepdim=True).value\n",
        "  #\n",
        "  # Start with (a) norm_logits = logits - logit_maxes\n",
        "  # norm_logits -> (32, 27)\n",
        "  # logits      -> (32, 27)\n",
        "  # An element by element operation, so local derivative is a unit tensor (all ones)\n",
        "  dlogits1 = torch.ones_like(norm_logits) * dnorm_logits\n",
        "\n",
        "\n",
        "  # b. logit_maxes = logits.max(1, keepdim=True).value\n",
        "  #\n",
        "  # logit_maxes -> (32,  1)\n",
        "  # logits      -> (32, 27)\n",
        "  #\n",
        "  # Intuitively, only the max values in each row are contributing to the gradient,\n",
        "  # rest all are zeros. So, we start with a zero-tensor (32, 27), update the indices\n",
        "  # of max-values with 1.0.\n",
        "  dlogits2 = torch.zeros_like(logits)\n",
        "  dlogits2[torch.arange(n), logits.max(1).indices] = 1.0\n",
        "  dlogits2 *= dlogit_maxes  # Local derivative * previous derivative\n",
        "  dlogits = dlogits2 + dlogits1 # Accrual of gradients\n",
        "  cmp('logits', dlogits, logits)\n",
        "\n",
        "  # 10. b2\n",
        "  # logits = h @ W2 + b2\n",
        "  # logits -> (32, 27)\n",
        "  # b2     -> (1,  27) --> broadcasted into (32, 27) (1 row replicated 32 times)\n",
        "  # We start with a (32, 27) unit tensor, multiply it with dlogits (let the\n",
        "  # gradients flow), we now have a (32, 27) tensor with gradients.\n",
        "  # What is the correct way to squash this into a (1, 27) tensor?\n",
        "  # local derivative = dlogits/db2\n",
        "  db2 = (torch.ones_like(logits) * dlogits).sum(0, keepdim=True)\n",
        "  cmp('b2', db2, b2)\n",
        "\n",
        "  # 11. h\n",
        "  # logits = h @ W2 + b2\n",
        "  # logits  -> (32, 27)\n",
        "  # h       -> (32, 64)\n",
        "  # W2      -> (64, 27)\n",
        "  # Each hij contributes to logits. We need to find exactly how it contributes\n",
        "  # and by how much.\n",
        "  # Because it is a dot product, every hij contributes wjk amount to logits-ik.\n",
        "  # - I believe the gradient dlogits is magnified/diminished through wjk.\n",
        "  # - So, every hij contributes a summation of gradient-contri * wjk\n",
        "  # - To find the gradient-contri, I need to understand how exactly does gradient\n",
        "  #   flow from logits to h.\n",
        "  # dlogits     -> (32, 27)\n",
        "  # W           -> (64, 27)\n",
        "  # W-Transpose -> (27, 64)\n",
        "  #\n",
        "  # The derivative is essentially dlogits * W-Transpose   (I took a lot of time\n",
        "  # to figure this out, on paper. AK showed it in a very simple manner)\n",
        "  #\n",
        "  dh = dlogits @ W2.T\n",
        "  cmp('h', dh, h)\n",
        "\n",
        "  # 12. W2\n",
        "  # logits = h @ W2 + b2\n",
        "  # logits  -> (32, 27)\n",
        "  # h       -> (32, 64)\n",
        "  # W2      -> (64, 27)\n",
        "  #\n",
        "  # dlogits -> (32, 27)\n",
        "  # h       -> (32, 64)\n",
        "  #\n",
        "  # Purely going by dimensionality, we do a h.T @ dlogits\n",
        "  # Checked on paper, similar to h but with a bit of difference\n",
        "  dW2 = h.T @ dlogits\n",
        "  cmp('W2', dW2, W2)\n",
        "\n",
        "  # 13. hpreact\n",
        "  # h = torch.tanh(hpreact)\n",
        "  # This is a simple element by element operation.\n",
        "  dhpreact = (1.0 - h**2) * dh\n",
        "  cmp('hpreact', dhpreact, hpreact)\n",
        "\n",
        "  # Got an approximate True for hpreact. Needs checking.\n",
        "  # I think from here, all gradients will be approximate.\n",
        "\n",
        "  # 14. bnbias\n",
        "  # hpreact = bngain * bnraw + bnbias\n",
        "  # hpreact -> (32, 64)\n",
        "  # bnbias  -> ( 1, 64) -> broadcasted into (32, 64)\n",
        "  # For the broadcasted tensor (32, 64), the local derivative is simply a (32, 64) unit tensor.\n",
        "  # d-broadcasted-tensor = local-derivative * dh (element by element multiplication)\n",
        "  #\n",
        "  # Then we add up along dimension 0 (adding all the gradients)\n",
        "  dbnbias = (torch.ones_like(hpreact) * dhpreact).sum(0, keepdim=True)\n",
        "  cmp('bnbias', dbnbias, bnbias)\n",
        "\n",
        "  # 15. bngain\n",
        "  # hpreact = bngain * bnraw + bnbias\n",
        "  # hpreact -> (32, 64)\n",
        "  # bngain  -> ( 1, 64) -> broadcasted to (32, 64), then multiplication.\n",
        "  # bnraw   -> (32, 64)\n",
        "  #\n",
        "  # Local derivative for the broadcasted tensor is simply bnraw, gradient is bnraw * dhpreact.\n",
        "  # Then we add the gradients along dimension 0 to get a (1, 64) tensor.\n",
        "  dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "  cmp('bngain', dbngain, bngain)\n",
        "\n",
        "  # 16. bnraw\n",
        "  # Simple multiplication of bngain * dhpreact\n",
        "  # bngain is broadcasted into a (32, 64)\n",
        "  dbnraw = bngain * dhpreact\n",
        "  cmp('bnraw', dbnraw, bnraw)\n",
        "\n",
        "  # 17. bndiff\n",
        "  # bnraw = bndiff * bnvar_inv\n",
        "  # bnraw -> (32, 64)\n",
        "  # bndiff -> (32, 64)\n",
        "  # bnvar_inv -> (1, 64) -> broadcasted into a (32, 64) and then element-level mul.\n",
        "  dbndiff_1 = dbnraw * bnvar_inv\n",
        "\n",
        "  # 18. bnvar_inv\n",
        "  dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "  cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "\n",
        "  # 19. bnvar\n",
        "  # bnvart_inv = (bnvar + 1e-5)**-0.5\n",
        "  dbnvar = (-0.5 * (bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "  cmp('bnvar', dbnvar, bnvar)\n",
        "\n",
        "  # 20. bndiff2\n",
        "  # bnvar = 1/(n-1) * (bndiff2).sum(0, keepdim=True)\n",
        "  # bnvar ->   ( 1, 64)\n",
        "  # bndiff2 -> (32, 64)\n",
        "  # We have seen that in a summation case, each element contributes unit gradient.\n",
        "  dbndiff2 = 1/(n-1) * torch.ones_like(bndiff2) * dbnvar\n",
        "  cmp('bndiff2', dbndiff2, bndiff2)\n",
        "\n",
        "  # 21. bndiff\n",
        "  # bndiff2 = bndiff**2\n",
        "  # Simple arithmetic op => element by element op.\n",
        "  dbndiff_2 = 2 * bndiff * dbndiff2\n",
        "  dbndiff = dbndiff_1 + dbndiff_2\n",
        "  cmp('bndiff', dbndiff, bndiff)\n",
        "\n",
        "  # 22. bnmeani\n",
        "  # bndiff = hprebn - bnmeani\n",
        "  # bndiff -> (32, 64)\n",
        "  # hprebn -> (32, 64)\n",
        "  # bnmeani-> ( 1, 64) -> broadcasted into (32, 64) and then subtracted.\n",
        "  # Take the broadcasted tensor first, each element in it contributes unit gradient,\n",
        "  # multiply that with dbndiff. Then sum it up along dimension 0 and multiply\n",
        "  # the whole thing by -1.0 (because bnmeani is being subtracted)\n",
        "  dbnmeani = (-1.0) * (torch.ones_like(bnmeani) * dbndiff).sum(0, keepdim=True)\n",
        "  cmp('bnmeani', dbnmeani, bnmeani)\n",
        "\n",
        "  # 23. hprebn\n",
        "  # bndiff = hprebn - bnmeani\n",
        "  # Simple unit gradient multiplied by dbndiff\n",
        "  dhprebn_1 = torch.ones_like(hprebn) * dbndiff\n",
        "\n",
        "  # bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "  dhprebn_2 = 1/n * torch.ones_like(hprebn) * dbnmeani\n",
        "\n",
        "  dhprebn = dhprebn_1 + dhprebn_2\n",
        "  cmp('hprebn', dhprebn, hprebn)\n",
        "\n",
        "\n",
        "  # 24. b1\n",
        "  # hprebn = embcat @ W1 + b1\n",
        "  # hprebn -> (32, 64)\n",
        "  # embcat -> (32, 30)\n",
        "  # W1     -> (30, 64)\n",
        "  # b1     -> ( 1, 64)\n",
        "  #\n",
        "  # Simply following the previous linear layer (refer b2/10)\n",
        "  db1 = (torch.ones_like(hprebn) * dhprebn).sum(0, keepdim=True)\n",
        "  cmp('b1', db1, b1)\n",
        "\n",
        "  # 25. W1\n",
        "  dW1 = embcat.T @ dhprebn\n",
        "  cmp('W1', dW1, W1)\n",
        "\n",
        "  # 26. embcat\n",
        "  dembcat = dhprebn @ W1.T\n",
        "  cmp('embcat', dembcat, embcat)\n",
        "\n",
        "  # 27. emb\n",
        "  demb = dembcat.view(emb.shape)\n",
        "  cmp('emb', demb, emb)\n",
        "\n",
        "  # 27. C\n",
        "  # C\n",
        "  # Understood from AK's explanation.\n",
        "  dC = torch.zeros_like(C)\n",
        "  for k in range(Xb.shape[0]):\n",
        "    for j in range(Xb.shape[1]):\n",
        "      ix = Xb[k, j]\n",
        "      dC[ix] += demb[k, j]\n",
        "  cmp('C', dC, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebLtYji_8PPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "095f364e-d86c-4b32-b61f-a9d99da55a7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.34928822517395 diff: 4.76837158203125e-07\n"
          ]
        }
      ],
      "source": [
        "## Exercise 2: backprop through cross_entropy but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the loss,\n",
        "# take the derivative, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdims=True)\n",
        "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gCXbB4C8PPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cbb7816-123c-45c9-c66e-81be0e4b0975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 5.820766091346741e-09\n"
          ]
        }
      ],
      "source": [
        "# backward pass\n",
        "\n",
        "dlogits = F.softmax(logits, 1)\n",
        "dlogits[range(n), Yb] -= 1\n",
        "dlogits /= n\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "#dlogits = None # TODO. my solution is 3 lines\n",
        "# -----------------\n",
        "\n",
        "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits[0] * n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOcFe-ED9mDU",
        "outputId": "ed86cebe-3fd6-4cc8-ceef-01f977baa023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0764,  0.0840,  0.0199,  0.0500,  0.0192,  0.0858,  0.0246,  0.0367,\n",
              "        -0.9826,  0.0304,  0.0357,  0.0382,  0.0351,  0.0271,  0.0336,  0.0131,\n",
              "         0.0096,  0.0192,  0.0154,  0.0523,  0.0506,  0.0232,  0.0237,  0.0700,\n",
              "         0.0606,  0.0265,  0.0218], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(dlogits.detach(), cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "Uik6nOLD9rjX",
        "outputId": "6aea95cb-3700-4d81-86a7-9fc2306e50a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7bb7b8a1c250>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd-MkhB68PPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85da4252-86ac-4caf-bd25-0e6a06e8ae2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ],
      "source": [
        "# Exercise 3: backprop through batchnorm but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
        "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
        "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "# bndiff2 = bndiff**2\n",
        "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "# hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# now:\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POdeZSKT8PPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1456c1ca-e097-4d0e-ad5a-da26edfd3eb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n"
          ]
        }
      ],
      "source": [
        "# backward pass\n",
        "\n",
        "# before we had:\n",
        "# dbnraw = bngain * dhpreact\n",
        "# dbndiff = bnvar_inv * dbnraw\n",
        "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "# dbndiff += (2*bndiff) * dbndiff2\n",
        "# dhprebn = dbndiff.clone()\n",
        "# dbnmeani = (-dbndiff).sum(0)\n",
        "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "\n",
        "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
        "# (you'll also need to use some of the variables from the forward pass up above)\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "dnprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum() - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "\n",
        "# -----------------\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPy8DhqB8PPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb64898a-b870-4738-c6fe-72fe1fa57bf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12297\n",
            "      0/ 200000: 3.7890\n",
            "  10000/ 200000: 2.2269\n",
            "  20000/ 200000: 2.3360\n",
            "  30000/ 200000: 2.4257\n",
            "  40000/ 200000: 1.9844\n",
            "  50000/ 200000: 2.3804\n",
            "  60000/ 200000: 2.4329\n",
            "  70000/ 200000: 2.1047\n",
            "  80000/ 200000: 2.3134\n",
            "  90000/ 200000: 2.1183\n",
            " 100000/ 200000: 1.9760\n",
            " 110000/ 200000: 2.3081\n",
            " 120000/ 200000: 1.9947\n",
            " 130000/ 200000: 2.5402\n",
            " 140000/ 200000: 2.2731\n",
            " 150000/ 200000: 2.1745\n",
            " 160000/ 200000: 2.0040\n",
            " 170000/ 200000: 1.8870\n",
            " 180000/ 200000: 2.1062\n",
            " 190000/ 200000: 1.8576\n"
          ]
        }
      ],
      "source": [
        "# Exercise 4: putting it all together!\n",
        "# Train the MLP neural net with your own backward pass\n",
        "\n",
        "# init\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written (TODO)\n",
        "with torch.no_grad():\n",
        "\n",
        "  # kick off optimization\n",
        "  for i in range(max_steps):\n",
        "\n",
        "    # minibatch construct\n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "    # forward pass\n",
        "    emb = C[Xb] # embed the characters into vectors\n",
        "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "    # Linear layer\n",
        "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "    # BatchNorm layer\n",
        "    # -------------------------------------------------------------\n",
        "    bnmean = hprebn.mean(0, keepdim=True)\n",
        "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
        "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
        "    hpreact = bngain * bnraw + bnbias\n",
        "    # -------------------------------------------------------------\n",
        "    # Non-linearity\n",
        "    h = torch.tanh(hpreact) # hidden layer\n",
        "    logits = h @ W2 + b2 # output layer\n",
        "    loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "    # backward pass\n",
        "    for p in parameters:\n",
        "      p.grad = None\n",
        "    # loss.backward() # use this for correctness comparisons, delete it later!\n",
        "\n",
        "    # manual backprop! #swole_doge_meme\n",
        "    # -----------------\n",
        "    # YOUR CODE HERE :)\n",
        "    dlogits = F.softmax(logits, 1)\n",
        "    dlogits[range(n), Yb] -= 1\n",
        "    dlogits /= n\n",
        "    # 2nd layer backprop\n",
        "    dh = dlogits @ W2.T\n",
        "    dW2 = h.T @ dlogits\n",
        "    db2 = dlogits.sum(0)\n",
        "    # tanh\n",
        "    dhpreact = (1.0 - h**2) * dh\n",
        "    # batchnorm backprop\n",
        "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "\n",
        "    #1st layer\n",
        "    dembcat = dhprebn @ W1.T\n",
        "    dW1 = embcat.T @ dhprebn\n",
        "    db1 = dhprebn.sum(0)\n",
        "    #embedding\n",
        "    demb = dembcat.view(emb.shape)\n",
        "    dC = torch.zeros_like(C)\n",
        "    for k in range(Xb.shape[0]):\n",
        "      for j in range(Xb.shape[1]):\n",
        "        ix = Xb[k, j]\n",
        "        dC[ix] += demb[k, j]\n",
        "    #dC, dW1, db1, dW2, db2, dbngain, dbnbias = None, None, None, None, None, None, None\n",
        "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "    # -----------------\n",
        "\n",
        "    # update\n",
        "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "    for p, grad in zip(parameters, grads):\n",
        "      # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
        "\n",
        "    # track stats\n",
        "    if i % 10000 == 0: # print every once in a while\n",
        "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "    lossi.append(loss.log10().item())\n",
        "\n",
        "    # if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
        "    #   break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEpI0hMW8PPz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "cef1cf43-d10d-42f4-c926-d83c400cbf8e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "all() received an invalid combination of arguments - got (bool), but expected one of:\n * (Tensor input, *, Tensor out = None)\n * (Tensor input, tuple of ints dim = None, bool keepdim = False, *, Tensor out = None)\n * (Tensor input, int dim, bool keepdim = False, *, Tensor out = None)\n * (Tensor input, name dim, bool keepdim = False, *, Tensor out = None)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-bfba7414151b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#useful for checking your gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mcmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-b3d747dfac40>\u001b[0m in \u001b[0;36mcmp\u001b[0;34m(s, dt, t)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# utility function we will use later when comparing manual gradients to PyTorch gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mapp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmaxdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: all() received an invalid combination of arguments - got (bool), but expected one of:\n * (Tensor input, *, Tensor out = None)\n * (Tensor input, tuple of ints dim = None, bool keepdim = False, *, Tensor out = None)\n * (Tensor input, int dim, bool keepdim = False, *, Tensor out = None)\n * (Tensor input, name dim, bool keepdim = False, *, Tensor out = None)\n"
          ]
        }
      ],
      "source": [
        "#useful for checking your gradients\n",
        "for p,g in zip(parameters, grads):\n",
        "  cmp(str(tuple(p.shape)), g, p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KImLWNoh8PP0"
      },
      "outputs": [],
      "source": [
        "# calibrate the batch norm at the end of training\n",
        "\n",
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aFnP_Zc8PP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7985db74-86ad-4246-bb94-b230fe21f1bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 2.678286075592041\n",
            "val 2.6766111850738525\n"
          ]
        }
      ],
      "source": [
        "# evaluate train and val loss\n",
        "\n",
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esWqmhyj8PP1"
      },
      "outputs": [],
      "source": [
        "# I achieved:\n",
        "# train 2.0718822479248047\n",
        "# val 2.1162495613098145"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHeQNv3s8PP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "568a2d8d-30a0-47dc-94ca-00f660940055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eria.\n",
            "kmyaz.\n",
            "hzen.\n",
            "edhn.\n",
            "amarethr.\n",
            "jekgrleg.\n",
            "azeredie.\n",
            "iulemi.\n",
            "jea.\n",
            "ekein.\n",
            "anwaarieltzi.\n",
            "hvqep.\n",
            "b.\n",
            "shdbn.\n",
            "gxhiries.\n",
            "kkkjrwellqhxntpocfnuaz.\n",
            "gvda.\n",
            "hylquemehs.\n",
            "kaajqhoflyan.\n",
            "hjan.\n"
          ]
        }
      ],
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # forward pass\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      if ix == 0:\n",
        "        break\n",
        "\n",
        "    print(''.join(itos[i] for i in out))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}